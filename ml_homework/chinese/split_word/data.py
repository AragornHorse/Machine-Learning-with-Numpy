import numpy as np
import json

# data path
pth = r"./data"

# sentences
with open(pth, 'r', encoding='utf-8') as f:
    data = [line[:-2] for line in f]

# whe bigger, the bigger punishment to long sentences
long_seq_punishment = 1.


# count every words' frequency
# auto generated by jieba
# using jieba just to count words' numbers, not for final split
def get_p():
    import jieba

    dic = {}

    for line in data:
        spl = list(jieba.cut(line, cut_all=False))
        for word in spl:
            try:
                dic[word] += 1
            except:
                dic[word] = 1

    print(len(dic))

    with open(r"./dic.json", 'w') as f:
        json.dump(dic, f)

# get_p()

# if the dictionary is too large, give up words exists only one time
def get_tiny_p():
    with open(r"./dic.json", 'r') as f:
        dic = json.load(f)

    dic_ = {}

    for key in dic.keys():
        if dic[key] > 3:
            dic_[key] = dic[key]

    # print(len(dic_))
    with open(r"./tiny_dic.json", 'w') as f:
        json.dump(dic_, f)

# get_tiny_p()


# load dictionary
with open(r"./dic.json", 'r') as f:
    dic = json.load(f)

N = 0
# print(dic)

# put frequency into possibility
for key in dic.keys():
    if dic[key] > N:
        N = dic[key]

N *= (1 + long_seq_punishment)


# weight of an edge is this word's possibility
def get_edge_weight(line: str):
    try:
        num = dic[line] / N
    except KeyError:
        if len(line) == 1:
            # one character, always can exist
            num = 1e-4
        else:
            num = 1e-60

    # print(line, num)

    return num


# print(get_edge_weight('hi'))

# using floyd to find the best way
def floyd_max_line(e):
    n = e.shape[0]
    D = np.copy(e)
    to = np.zeros([n, n])
    for i in range(n):
        for j in range(n):
            to[i, j] = j

    for k in range(n):
        for i in range(n):
            for j in range(n):
                if i >= j or k >= j or i >= k:
                    continue

                if D[i, k] == 0 or D[k, j] == 0:
                    continue

                # print(D[i, k], D[k, j], D[i, j], split_seq("你在干啥呢啊啊啊", [i, k, j]))

                # p(ab) = p(a)p(b)
                if D[i, k] * D[k, j] >= D[i, j]:
                    # print("ok\n")
                    # print(D)
                    # print(i, j, k)
                    # print('')
                    to[i, j] = to[i, k]
                    D[i, j] = D[i, k] * D[k, j]

    return to, D


# generate graph
def get_graph(line):
    n = len(line) + 1
    e = np.zeros([n, n])

    for i in range(n):
        for j in range(n):
            if i >= j:
                e[i, j] = -10000
            else:
                word = line[i:j]
                # print(i, j, word)
                e[i, j] = get_edge_weight(word)
    return e


# e = get_graph('是谁啊啊啊')
# to, _ = floyd_max_line(e)
# print(to)

# get the path from floyd result
def get_line(to, i=0, j=None):
    if j is None:
        j = to.shape[0] - 1

    now = i

    line = [i]

    while now < j:
        line.append(int(to[int(now), int(j)]))
        now = int(to[int(now), int(j)])

    return line


# split a sentence by line
def split_seq(line, path):
    lst = []
    last = path[0]
    for t in path[1:]:
        lst.append(line[last: t])
        last = t

    return lst


# final function
def split_sentence(sen):

    e = get_graph(sen)
    to, _ = floyd_max_line(e)

    line = get_line(to)

    rst = split_seq(sen, line)

    return rst


# split 1000 sentences
with open(r"./rst.txt", 'w', encoding='utf-8') as f:
    for line in data[:1000]:
        f.write(line + '\n')
        rst = split_sentence(line)
        for r in rst:
            f.write(r + ',')
        f.write("\n\n")

